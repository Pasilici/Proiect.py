import requests
from bs4 import BeautifulSoup
from tkinter import *
from tkcalendar import Calendar
import pandas as pd
import re
import os
import numpy
import cv2
from selenium import webdriver
import json


BASE_URL = 'http://ro.tntimisoara.com/category/stagiunea_18-19/'


my_headers = {"User-Agent": "Microsoft Edge.Ink"}
page = requests.get(BASE_URL, headers=my_headers)

soup = BeautifulSoup(page.content, 'html.parser')

# print(soup.prettify())

labs = re.findall(r'((?<=<h3>)(.+?)(?=</h3>))', soup.prettify())

headings = soup.find_all('h3')

f = open("labs.txt", "w")

for heading in headings:
    f.write(heading.text)

f.close()

root = Tk()

root.geometry("400x400")

cal = Calendar(root, selectmode='day', year=2021, month=5, day=4)
cal.pack(pady=20)


def grad_date():
    date.config(text="Data selectata:" + cal.get_date())


Button(root, text="Selectati data", command=grad_date).pack(pady=20)

date = Label(root, text="")
date.pack(pady=20)

root.mainloop()

BASE_URL = 'http://ro.tntimisoara.com/category/stagiunea_18-19/'


my_headers = {"User-Agent": "Microsoft Edge.Ink"}
page = requests.get(BASE_URL, headers=my_headers)

soup = BeautifulSoup(page.content, 'html.parser')

content = soup.find_all('div', {'class': 'sidebar_content'})

for item in content:
    urluri = soup.find_all('h3', {'class': 'cufon'})

    d = open('links.txt', 'w')
    print(urluri, file=d)
    d = open('links.txt', 'r')

    z = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', d.read())
    g = open('linkuri_piese.txt', 'w')
    print(z, file=g)

for url in 'linkuri_piese.txt':
    my_headers = {"User-Agent": "Microsoft Edge.Ink"}
    page = requests.get(BASE_URL, headers=my_headers)

    soup = BeautifulSoup(page.content, 'html.parser')

    content1 = soup.find_all('div', {'class': 'sidebar_content'})
    content2 = str(content1)

    locatia  = re.findall('[S]\S\S\S\s\d' , content2)
    data = re.findall('\d\d.\d\d.\d\d\d\d', content2)
    print(locatia)
    print(data)

GOOGLE_IMAGE = \
               'http://ro.tntimisoara.com/category/stagiunea_18-19/'

# The User-Agent request header contains a characteristic string
# that allows the network protocol peers to identify the application type,
# operating system, and software version of the requesting software user agent.
# needed for google search
usr_agent = {
    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',
    'Accept-Encoding': 'none',
    'Accept-Language': 'en-US,en;q=0.8',
    'Connection': 'keep-alive',
}

SAVE_FOLDER = 'images'


def main():
    if not os.path.exists(SAVE_FOLDER):
        os.mkdir(SAVE_FOLDER)
    download_images()


def download_images():
    # ask for user input
    data = input('What are you looking for? ')
    n_images = int(input('How many images do you want? '))

    print('Start searching...')

    # get url query string
    searchurl = GOOGLE_IMAGE + 'q=' + data
    print(searchurl)

    # request url, without usr_agent the permission gets denied
    response = requests.get(searchurl, headers=usr_agent)
    html = response.text

    # find all divs where class='rg_meta'
    soup = BeautifulSoup(html, 'html.parser')
    results = soup.findAll('src', {'src': ' '}, limit=n_images)

    # extract the link from the div tag
    imagelinks = []
    for re in results:
        text = re.text  # this is a valid json string
        text_dict = json.loads(text)  # deserialize json to a Python dict
        link = text_dict['ou']
        # image_type = text_dict['ity']
        imagelinks.append(link)

    print(f'found {len(imagelinks)} images')
    print('Start downloading...')

    for i, imagelink in enumerate(imagelinks):
        # open image link and save as file
        response = requests.get(imagelink)

        imagename = SAVE_FOLDER + '/' + data + str(i + 1) + '.jpg'
        with open(imagename, 'wb') as file:
            file.write(response.content)

    print('Done')


if __name__ == '__main__':
    main()
